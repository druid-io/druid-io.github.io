<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Apache Druid">
<meta name="keywords" content="druid,kafka,database,analytics,streaming,real-time,real time,apache,open source">
<meta name="author" content="Apache Software Foundation">

<title>Druid | </title>

<link rel="alternate" type="application/atom+xml" href="/feed">
<link rel="shortcut icon" href="/img/favicon.png">

<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">

<link href='//fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700,300italic|Open+Sans:300italic,400italic,600italic,400,300,600,700' rel='stylesheet' type='text/css'>

<link rel="stylesheet" href="/css/bootstrap-pure.css?v=1.0">
<link rel="stylesheet" href="/css/main.css?v=1.0">
<link rel="stylesheet" href="/css/header.css?v=1.0">
<link rel="stylesheet" href="/css/footer.css?v=1.0">
<link rel="stylesheet" href="/css/syntax.css?v=1.0">
<link rel="stylesheet" href="/css/docs.css?v=1.0">

<script>
  (function() {
    var cx = '000162378814775985090:molvbm0vggm';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>


  </head>

  <body>
    <!-- Start page_header include -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>

<div class="top-navigator">
  <div class="container">
    <div class="left-cont">
      <a class="logo" href="/"><span class="druid-logo"></span></a>
    </div>
    <div class="right-cont">
      <ul class="links">
        <li class=""><a href="/technology">Technology</a></li>
        <li class=""><a href="/use-cases">Use Cases</a></li>
        <li class=""><a href="/druid-powered">Powered By</a></li>
        <li class=""><a href="/docs/latest/design/">Docs</a></li>
        <li class=""><a href="https://druid.apache.org/community/">Community</a></li>
        <li class=" button-link"><a href="/downloads.html">Download</a></li>
      </ul>
    </div>
  </div>
  <div class="action-button menu-icon">
    <span class="fa fa-bars"></span> MENU
  </div>
  <div class="action-button menu-icon-close">
    <span class="fa fa-times"></span> MENU
  </div>
</div>

<script type="text/javascript">
  var $menu = $('.right-cont');
  var $menuIcon = $('.menu-icon');
  var $menuIconClose = $('.menu-icon-close');

  function showMenu() {
    $menu.fadeIn(100);
    $menuIcon.fadeOut(100);
    $menuIconClose.fadeIn(100);
  }

  $menuIcon.click(showMenu);

  function hideMenu() {
    $menu.fadeOut(100);
    $menuIconClose.fadeOut(100);
    $menuIcon.fadeIn(100);
  }

  $menuIconClose.click(hideMenu);

  $(window).resize(function() {
    if ($(window).width() >= 840) {
      $menu.fadeIn(100);
      $menuIcon.fadeOut(100);
      $menuIconClose.fadeOut(100);
    }
    else {
      $menu.fadeOut(100);
      $menuIcon.fadeIn(100);
      $menuIconClose.fadeOut(100);
    }
  });
</script>

<!-- Stop page_header include -->


    <div class="container doc-container">
      
      

      
      <p> Looking for the <a href="/docs/0.14.0-incubating/">latest stable documentation</a>?</p>
      

      <div class="row">
        <div class="col-md-9 doc-content">
          <p>
            <a class="btn btn-default btn-xs visible-xs-inline-block visible-sm-inline-block" href="#toc">Table of Contents</a>
          </p>
          <h1 id="batch-data-ingestion">Batch Data Ingestion</h1>

<p>There are two choices for batch data ingestion to your Druid cluster, you can use the <a href="Indexing-Service.html">Indexing service</a> or you can use the <code>HadoopDruidIndexer</code>.</p>

<h2 id="which-should-i-use">Which should I use?</h2>

<p>The <a href="Indexing-Service.html">Indexing service</a> is a set of nodes that can run as part of your Druid cluster and can accomplish a number of different types of indexing tasks. Even if all you care about is batch indexing, it provides for the encapsulation of things like the <a href="Metadata-storage.html">metadata store</a> that is used for segment metadata and other things, so that your indexing tasks do not need to include such information. The indexing service was created such that external systems could programmatically interact with it and run periodic indexing tasks. Long-term, the indexing service is going to be the preferred method of ingesting data.</p>

<p>The <code>HadoopDruidIndexer</code> runs hadoop jobs in order to separate and index data segments. It takes advantage of Hadoop as a job scheduling and distributed job execution platform. It is a simple method if you already have Hadoop running and don’t want to spend the time configuring and deploying the <a href="Indexing-Service.html">Indexing service</a> just yet.</p>

<h2 id="batch-ingestion-using-the-hadoopdruidindexer">Batch Ingestion using the HadoopDruidIndexer</h2>

<p>The HadoopDruidIndexer can be run like so:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:&lt;hadoop_config_path&gt; io.druid.cli.Main index hadoop &lt;spec_file&gt;
</code></pre></div>
<h2 id="hadoop-specfile">Hadoop &quot;specFile&quot;</h2>

<p>The spec_file is a path to a file that contains JSON and an example looks like:</p>
<div class="highlight"><pre><code class="language-json" data-lang="json"><span></span><span class="p">{</span>
  <span class="nt">&quot;dataSchema&quot;</span> <span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;dataSource&quot;</span> <span class="p">:</span> <span class="s2">&quot;wikipedia&quot;</span><span class="p">,</span>
    <span class="nt">&quot;parser&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
      <span class="nt">&quot;parseSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;format&quot;</span> <span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span>
        <span class="nt">&quot;timestampSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
          <span class="nt">&quot;column&quot;</span> <span class="p">:</span> <span class="s2">&quot;timestamp&quot;</span><span class="p">,</span>
          <span class="nt">&quot;format&quot;</span> <span class="p">:</span> <span class="s2">&quot;auto&quot;</span>
        <span class="p">},</span>
        <span class="nt">&quot;dimensionsSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
          <span class="nt">&quot;dimensions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;page&quot;</span><span class="p">,</span><span class="s2">&quot;language&quot;</span><span class="p">,</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;unpatrolled&quot;</span><span class="p">,</span><span class="s2">&quot;newPage&quot;</span><span class="p">,</span><span class="s2">&quot;robot&quot;</span><span class="p">,</span><span class="s2">&quot;anonymous&quot;</span><span class="p">,</span><span class="s2">&quot;namespace&quot;</span><span class="p">,</span><span class="s2">&quot;continent&quot;</span><span class="p">,</span><span class="s2">&quot;country&quot;</span><span class="p">,</span><span class="s2">&quot;region&quot;</span><span class="p">,</span><span class="s2">&quot;city&quot;</span><span class="p">],</span>
          <span class="nt">&quot;dimensionExclusions&quot;</span> <span class="p">:</span> <span class="p">[],</span>
          <span class="nt">&quot;spatialDimensions&quot;</span> <span class="p">:</span> <span class="p">[]</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">},</span>
    <span class="nt">&quot;metricsSpec&quot;</span> <span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;count&quot;</span><span class="p">,</span>
        <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;count&quot;</span>
      <span class="p">},</span>
      <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
        <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;added&quot;</span><span class="p">,</span>
        <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;added&quot;</span>
      <span class="p">},</span>
      <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
        <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;deleted&quot;</span><span class="p">,</span>
        <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;deleted&quot;</span>
      <span class="p">},</span>
      <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
        <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;delta&quot;</span><span class="p">,</span>
        <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;delta&quot;</span>
      <span class="p">}</span>
    <span class="p">],</span>
    <span class="nt">&quot;granularitySpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;uniform&quot;</span><span class="p">,</span>
      <span class="nt">&quot;segmentGranularity&quot;</span> <span class="p">:</span> <span class="s2">&quot;DAY&quot;</span><span class="p">,</span>
      <span class="nt">&quot;queryGranularity&quot;</span> <span class="p">:</span> <span class="s2">&quot;NONE&quot;</span><span class="p">,</span>
      <span class="nt">&quot;intervals&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;2013-08-31/2013-09-01&quot;</span> <span class="p">]</span>
    <span class="p">}</span>
  <span class="p">},</span>
  <span class="nt">&quot;ioConfig&quot;</span> <span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;hadoop&quot;</span><span class="p">,</span>
    <span class="nt">&quot;inputSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;static&quot;</span><span class="p">,</span>
      <span class="nt">&quot;paths&quot;</span> <span class="p">:</span> <span class="s2">&quot;/MyDirectory/examples/indexing/wikipedia_data.json&quot;</span>
    <span class="p">},</span>
    <span class="nt">&quot;metadataUpdateSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;metadata&quot;</span><span class="p">,</span>
      <span class="nt">&quot;connectURI&quot;</span> <span class="p">:</span> <span class="s2">&quot;jdbc:metadata storage://localhost:3306/druid&quot;</span><span class="p">,</span>
      <span class="nt">&quot;password&quot;</span> <span class="p">:</span> <span class="s2">&quot;diurd&quot;</span><span class="p">,</span>
      <span class="nt">&quot;segmentTable&quot;</span> <span class="p">:</span> <span class="s2">&quot;druid_segments&quot;</span><span class="p">,</span>
      <span class="nt">&quot;user&quot;</span> <span class="p">:</span> <span class="s2">&quot;druid&quot;</span>
    <span class="p">},</span>
    <span class="nt">&quot;segmentOutputPath&quot;</span> <span class="p">:</span> <span class="s2">&quot;/MyDirectory/data/index/output&quot;</span>
  <span class="p">},</span>
  <span class="nt">&quot;tuningConfig&quot;</span> <span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;hadoop&quot;</span><span class="p">,</span>
    <span class="nt">&quot;workingPath&quot;</span><span class="p">:</span> <span class="s2">&quot;/tmp&quot;</span><span class="p">,</span>
    <span class="nt">&quot;partitionsSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span>
      <span class="nt">&quot;partitionDimension&quot;</span> <span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
      <span class="nt">&quot;targetPartitionSize&quot;</span> <span class="p">:</span> <span class="mi">5000000</span><span class="p">,</span>
      <span class="nt">&quot;maxPartitionSize&quot;</span> <span class="p">:</span> <span class="mi">7500000</span><span class="p">,</span>
      <span class="nt">&quot;assumeGrouped&quot;</span> <span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
      <span class="nt">&quot;numShards&quot;</span> <span class="p">:</span> <span class="mi">-1</span>
    <span class="p">},</span>
    <span class="nt">&quot;shardSpecs&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="p">},</span>
    <span class="nt">&quot;leaveIntermediate&quot;</span> <span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">&quot;cleanupOnFailure&quot;</span> <span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
    <span class="nt">&quot;overwriteFiles&quot;</span> <span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">&quot;ignoreInvalidRows&quot;</span> <span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">&quot;jobProperties&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="p">},</span>
    <span class="nt">&quot;combineText&quot;</span> <span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">&quot;persistInHeap&quot;</span> <span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">&quot;ingestOffheap&quot;</span> <span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">&quot;bufferSize&quot;</span> <span class="p">:</span> <span class="mi">134217728</span><span class="p">,</span>
    <span class="nt">&quot;aggregationBufferRatio&quot;</span> <span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="nt">&quot;rowFlushBoundary&quot;</span> <span class="p">:</span> <span class="mi">300000</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="dataschema">DataSchema</h3>

<p>This field is required.</p>

<p>See <a href="Ingestion.html">Ingestion</a></p>

<h3 id="ioconfig">IOConfig</h3>

<p>This field is required.</p>

<table><thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
<th>Required</th>
</tr>
</thead><tbody>
<tr>
<td>type</td>
<td>String</td>
<td>This should always be &#39;hadoop&#39;.</td>
<td>yes</td>
</tr>
<tr>
<td>pathSpec</td>
<td>Object</td>
<td>a specification of where to pull the data in from</td>
<td>yes</td>
</tr>
<tr>
<td>segmentOutputPath</td>
<td>String</td>
<td>the path to dump segments into.</td>
<td>yes</td>
</tr>
<tr>
<td>metadataUpdateSpec</td>
<td>Object</td>
<td>a specification of how to update the metadata for the druid cluster these segments belong to.</td>
<td>yes</td>
</tr>
</tbody></table>

<h4 id="path-specification">Path specification</h4>

<p>There are multiple types of path specification:</p>

<h5 id="static"><code>static</code></h5>

<p>Is a type of data loader where a static path to where the data files are located is passed.</p>

<table><thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
<th>Required</th>
</tr>
</thead><tbody>
<tr>
<td>paths</td>
<td>Array of String</td>
<td>A String of input paths indicating where the raw data is located.</td>
<td>yes</td>
</tr>
</tbody></table>

<p>For example, using the static input paths:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>&quot;paths&quot; : &quot;s3n://billy-bucket/the/data/is/here/data.gz, s3n://billy-bucket/the/data/is/here/moredata.gz, s3n://billy-bucket/the/data/is/here/evenmoredata.gz&quot;
</code></pre></div>
<h5 id="granularity"><code>granularity</code></h5>

<p>Is a type of data loader that expects data to be laid out in a specific path format. Specifically, it expects it to be segregated by day in this directory format <code>y=XXXX/m=XX/d=XX/H=XX/M=XX/S=XX</code> (dates are represented by lowercase, time is represented by uppercase).</p>

<table><thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
<th>Required</th>
</tr>
</thead><tbody>
<tr>
<td>dataGranularity</td>
<td>Object</td>
<td>specifies the granularity to expect the data at, e.g. hour means to expect directories <code>y=XXXX/m=XX/d=XX/H=XX</code>.</td>
<td>yes</td>
</tr>
<tr>
<td>inputPath</td>
<td>String</td>
<td>Base path to append the expected time path to.</td>
<td>yes</td>
</tr>
<tr>
<td>filePattern</td>
<td>String</td>
<td>Pattern that files should match to be included.</td>
<td>yes</td>
</tr>
</tbody></table>

<p>For example, if the sample config were run with the interval 2012-06-01/2012-06-02, it would expect data at the paths</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=00
s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=01
...
s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=23
</code></pre></div>
<h4 id="metadata-update-job-spec">Metadata Update Job Spec</h4>

<p>This is a specification of the properties that tell the job how to update metadata such that the Druid cluster will see the output segments and load them.</p>

<table><thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
<th>Required</th>
</tr>
</thead><tbody>
<tr>
<td>type</td>
<td>String</td>
<td>&quot;metadata&quot; is the only value available.</td>
<td>yes</td>
</tr>
<tr>
<td>connectURI</td>
<td>String</td>
<td>A valid JDBC url to metadata storage.</td>
<td>yes</td>
</tr>
<tr>
<td>user</td>
<td>String</td>
<td>Username for db.</td>
<td>yes</td>
</tr>
<tr>
<td>password</td>
<td>String</td>
<td>password for db.</td>
<td>yes</td>
</tr>
<tr>
<td>segmentTable</td>
<td>String</td>
<td>Table to use in DB.</td>
<td>yes</td>
</tr>
</tbody></table>

<p>These properties should parrot what you have configured for your <a href="Coordinator.html">Coordinator</a>.</p>

<h3 id="tuningconfig">TuningConfig</h3>

<p>The tuningConfig is optional and default parameters will be used if no tuningConfig is specified.</p>

<table><thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
<th>Required</th>
</tr>
</thead><tbody>
<tr>
<td>workingPath</td>
<td>String</td>
<td>the working path to use for intermediate results (results between Hadoop jobs).</td>
<td>no (default == &#39;/tmp/druid-indexing&#39;)</td>
</tr>
<tr>
<td>version</td>
<td>String</td>
<td>The version of created segments.</td>
<td>no (default == datetime that indexing starts at)</td>
</tr>
<tr>
<td>leaveIntermediate</td>
<td>leave behind files in the workingPath when job completes or fails (debugging tool).</td>
<td>no (default == false)</td>
<td></td>
</tr>
<tr>
<td>partitionsSpec</td>
<td>Object</td>
<td>a specification of how to partition each time bucket into segments, absence of this property means no partitioning will occur.More details below.</td>
<td>no (default == &#39;hashed&#39;</td>
</tr>
<tr>
<td>maxRowsInMemory</td>
<td>Integer</td>
<td>The number of rows to aggregate before persisting. This number is the post-aggregation rows, so it is not equivalent to the number of input events, but the number of aggregated rows that those events result in. This is used to manage the required JVM heap size.</td>
<td>no (default == 5 million)</td>
</tr>
<tr>
<td>cleanupOnFailure</td>
<td>Boolean</td>
<td>Cleans up intermediate files when the job fails as opposed to leaving them around for debugging.</td>
<td>no (default == true)</td>
</tr>
<tr>
<td>overwriteFiles</td>
<td>Boolean</td>
<td>Override existing files found during indexing.</td>
<td>no (default == false)</td>
</tr>
<tr>
<td>ignoreInvalidRows</td>
<td>Boolean</td>
<td>Ignore rows found to have problems.</td>
<td>no (default == false)</td>
</tr>
<tr>
<td>jobProperties</td>
<td>Object</td>
<td>a map of properties to add to the Hadoop job configuration.</td>
<td>no (default == null)</td>
</tr>
</tbody></table>

<h3 id="partitioning-specification">Partitioning specification</h3>

<p>Segments are always partitioned based on timestamp (according to the granularitySpec) and may be further partitioned in
some other way depending on partition type. Druid supports two types of partitioning strategies: &quot;hashed&quot; (based on the
hash of all dimensions in each row), and &quot;dimension&quot; (based on ranges of a single dimension).</p>

<p>Hashed partitioning is recommended in most cases, as it will improve indexing performance and create more uniformly
sized data segments relative to single-dimension partitioning.</p>

<h4 id="hash-based-partitioning">Hash-based partitioning</h4>
<div class="highlight"><pre><code class="language-json" data-lang="json"><span></span>  <span class="s2">&quot;partitionsSpec&quot;</span><span class="err">:</span> <span class="p">{</span>
     <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;hashed&quot;</span><span class="p">,</span>
     <span class="nt">&quot;targetPartitionSize&quot;</span><span class="p">:</span> <span class="mi">5000000</span>
   <span class="p">}</span>
</code></pre></div>
<p>Hashed partitioning works by first selecting a number of segments, and then partitioning rows across those segments
according to the hash of all dimensions in each row. The number of segments is determined automatically based on the
cardinality of the input set and a target partition size.</p>

<p>The configuration options are:</p>

<table><thead>
<tr>
<th>property</th>
<th>description</th>
<th>required?</th>
</tr>
</thead><tbody>
<tr>
<td>type</td>
<td>type of partitionSpec to be used</td>
<td>&quot;hashed&quot;</td>
</tr>
<tr>
<td>targetPartitionSize</td>
<td>target number of rows to include in a partition, should be a number that targets segments of 500MB~1GB.</td>
<td>either this or numShards</td>
</tr>
<tr>
<td>numShards</td>
<td>specify the number of partitions directly, instead of a target partition size. Ingestion will run faster, since it can skip the step necessary to select a number of partitions automatically.</td>
<td>either this or targetPartitionSize</td>
</tr>
</tbody></table>

<h4 id="single-dimension-partitioning">Single-dimension partitioning</h4>
<div class="highlight"><pre><code class="language-json" data-lang="json"><span></span>  <span class="s2">&quot;partitionsSpec&quot;</span><span class="err">:</span> <span class="p">{</span>
     <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span>
     <span class="nt">&quot;targetPartitionSize&quot;</span><span class="p">:</span> <span class="mi">5000000</span>
   <span class="p">}</span>
</code></pre></div>
<p>Single-dimension partitioning works by first selecting a dimension to partition on, and then separating that dimension
into contiguous ranges. Each segment will contain all rows with values of that dimension in that range. For example,
your segments may be partitioned on the dimension &quot;host&quot; using the ranges &quot;a.example.com&quot; to &quot;f.example.com&quot; and
&quot;f.example.com&quot; to &quot;z.example.com&quot;. By default, the dimension to use is determined automatically, although you can
override it with a specific dimension.</p>

<p>The configuration options are:</p>

<table><thead>
<tr>
<th>property</th>
<th>description</th>
<th>required?</th>
</tr>
</thead><tbody>
<tr>
<td>type</td>
<td>type of partitionSpec to be used</td>
<td>&quot;dimension&quot;</td>
</tr>
<tr>
<td>targetPartitionSize</td>
<td>target number of rows to include in a partition, should be a number that targets segments of 500MB~1GB.</td>
<td>yes</td>
</tr>
<tr>
<td>maxPartitionSize</td>
<td>maximum number of rows to include in a partition. Defaults to 50% larger than the targetPartitionSize.</td>
<td>no</td>
</tr>
<tr>
<td>partitionDimension</td>
<td>the dimension to partition on. Leave blank to select a dimension automatically.</td>
<td>no</td>
</tr>
<tr>
<td>assumeGrouped</td>
<td>assume input data has already been grouped on time and dimensions. Ingestion will run faster, but can choose suboptimal partitions if the assumption is violated.</td>
<td>no</td>
</tr>
</tbody></table>

<h2 id="batch-ingestion-using-the-indexing-service">Batch Ingestion Using the Indexing Service</h2>

<p>Batch ingestion for the indexing service is done by submitting an <a href="Tasks.html">Index Task</a> (for datasets &lt; 1G) or a <a href="Tasks.html">Hadoop Index Task</a>. The indexing service can be started by issuing:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>java -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:config/overlord io.druid.cli.Main server overlord
</code></pre></div>
<p>This will start up a very simple local indexing service. For more complex deployments of the indexing service, see <a href="Indexing-Service.html">here</a>.</p>

<p>The schema of the Hadoop Index Task contains a task &quot;type&quot; and a Hadoop Index Config. A sample Hadoop index task is shown below:</p>
<div class="highlight"><pre><code class="language-json" data-lang="json"><span></span><span class="p">{</span>
  <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;index_hadoop&quot;</span><span class="p">,</span>
  <span class="nt">&quot;spec&quot;</span> <span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;dataSchema&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;dataSource&quot;</span> <span class="p">:</span> <span class="s2">&quot;wikipedia&quot;</span><span class="p">,</span>
      <span class="nt">&quot;parser&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="nt">&quot;parseSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
          <span class="nt">&quot;format&quot;</span> <span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span>
          <span class="nt">&quot;timestampSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
            <span class="nt">&quot;column&quot;</span> <span class="p">:</span> <span class="s2">&quot;timestamp&quot;</span><span class="p">,</span>
            <span class="nt">&quot;format&quot;</span> <span class="p">:</span> <span class="s2">&quot;auto&quot;</span>
          <span class="p">},</span>
          <span class="nt">&quot;dimensionsSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
            <span class="nt">&quot;dimensions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;page&quot;</span><span class="p">,</span><span class="s2">&quot;language&quot;</span><span class="p">,</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;unpatrolled&quot;</span><span class="p">,</span><span class="s2">&quot;newPage&quot;</span><span class="p">,</span><span class="s2">&quot;robot&quot;</span><span class="p">,</span><span class="s2">&quot;anonymous&quot;</span><span class="p">,</span><span class="s2">&quot;namespace&quot;</span><span class="p">,</span><span class="s2">&quot;continent&quot;</span><span class="p">,</span><span class="s2">&quot;country&quot;</span><span class="p">,</span><span class="s2">&quot;region&quot;</span><span class="p">,</span><span class="s2">&quot;city&quot;</span><span class="p">],</span>
            <span class="nt">&quot;dimensionExclusions&quot;</span> <span class="p">:</span> <span class="p">[],</span>
            <span class="nt">&quot;spatialDimensions&quot;</span> <span class="p">:</span> <span class="p">[]</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">},</span>
      <span class="nt">&quot;metricsSpec&quot;</span> <span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
          <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;count&quot;</span><span class="p">,</span>
          <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;count&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
          <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
          <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;added&quot;</span><span class="p">,</span>
          <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;added&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
          <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
          <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;deleted&quot;</span><span class="p">,</span>
          <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;deleted&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
          <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
          <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;delta&quot;</span><span class="p">,</span>
          <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;delta&quot;</span>
        <span class="p">}</span>
      <span class="p">],</span>
      <span class="nt">&quot;granularitySpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;uniform&quot;</span><span class="p">,</span>
        <span class="nt">&quot;segmentGranularity&quot;</span> <span class="p">:</span> <span class="s2">&quot;DAY&quot;</span><span class="p">,</span>
        <span class="nt">&quot;queryGranularity&quot;</span> <span class="p">:</span> <span class="s2">&quot;NONE&quot;</span><span class="p">,</span>
        <span class="nt">&quot;intervals&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;2013-08-31/2013-09-01&quot;</span> <span class="p">]</span>
      <span class="p">}</span>
    <span class="p">},</span>
    <span class="nt">&quot;ioConfig&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;hadoop&quot;</span><span class="p">,</span>
      <span class="nt">&quot;inputSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;static&quot;</span><span class="p">,</span>
        <span class="nt">&quot;paths&quot;</span> <span class="p">:</span> <span class="s2">&quot;/MyDirectory/examples/indexing/wikipedia_data.json&quot;</span>
      <span class="p">}</span>
    <span class="p">},</span>
    <span class="nt">&quot;tuningConfig&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;hadoop&quot;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="dataschema">DataSchema</h3>

<p>This field is required.</p>

<p>See <a href="Ingestion.html">Ingestion</a></p>

<h3 id="ioconfig">IOConfig</h3>

<p>This field is required.</p>

<table><thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
<th>Required</th>
</tr>
</thead><tbody>
<tr>
<td>type</td>
<td>String</td>
<td>This should always be &#39;hadoop&#39;.</td>
<td>yes</td>
</tr>
<tr>
<td>pathSpec</td>
<td>Object</td>
<td>a specification of where to pull the data in from</td>
<td>yes</td>
</tr>
</tbody></table>

<h3 id="tuningconfig">TuningConfig</h3>

<p>The tuningConfig is optional and default parameters will be used if no tuningConfig is specified. This is the same as the tuningConfig for the standalone Hadoop indexer. See above for more details.</p>

<h3 id="running-the-task">Running the Task</h3>

<p>The Hadoop Index Config submitted as part of an Hadoop Index Task is identical to the Hadoop Index Config used by the <code>HadoopBatchIndexer</code> except that three fields must be omitted: <code>segmentOutputPath</code>, <code>workingPath</code>, <code>updaterJobSpec</code>. The Indexing Service takes care of setting these fields internally.</p>

<p>To run the task:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>curl -X &#39;POST&#39; -H &#39;Content-Type:application/json&#39; -d @example_index_hadoop_task.json localhost:8087/druid/indexer/v1/task
</code></pre></div>
<p>If the task succeeds, you should see in the logs of the indexing service:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>2013-10-16 16:38:31,945 INFO [pool-6-thread-1] io.druid.indexing.overlord.exec.TaskConsumer - Task SUCCESS: HadoopIndexTask...
</code></pre></div>
<h2 id="having-problems">Having Problems?</h2>

<p>Getting data into Druid can definitely be difficult for first time users. Please don&#39;t hesitate to ask questions in our IRC channel or on our <a href="https://groups.google.com/forum/#!forum/druid-development">google groups page</a>.</p>

        </div>
        <div class="col-md-3">
          <div class="searchbox">
            <gcse:searchbox-only></gcse:searchbox-only>
          </div>
          <div id="toc" class="nav toc hidden-print">
          </div>
        </div>
      </div>
    </div>

    <!-- Start page_footer include -->
<footer class="druid-footer">
<div class="container">
  <div class="text-center">
    <p>
    <a href="/technology">Technology</a>&ensp;·&ensp;
    <a href="/use-cases">Use Cases</a>&ensp;·&ensp;
    <a href="/druid-powered">Powered by Druid</a>&ensp;·&ensp;
    <a href="/docs/latest">Docs</a>&ensp;·&ensp;
    <a href="https://druid.apache.org/community/">Community</a>&ensp;·&ensp;
    <a href="/downloads.html">Download</a>&ensp;·&ensp;
    <a href="/faq">FAQ</a>
    </p>
  </div>
  <div class="text-center">
    <a title="Join the user group" href="https://groups.google.com/forum/#!forum/druid-user" target="_blank"><span class="fa fa-comments"></span></a>&ensp;·&ensp;
    <a title="Follow Druid" href="https://twitter.com/druidio" target="_blank"><span class="fab fa-twitter"></span></a>&ensp;·&ensp;
    <a title="Download via Apache" href="https://www.apache.org/dyn/closer.cgi?path=/incubator/druid/0.14.0-incubating/apache-druid-0.14.0-incubating-bin.tar.gz" target="_blank"><span class="fas fa-feather"></span></a>&ensp;·&ensp;
    <a title="GitHub" href="https://github.com/apache/incubator-druid" target="_blank"><span class="fab fa-github"></span></a>
  </div>
  <div class="text-center license">
    Except where otherwise noted, licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>
  </div>
</div>
</footer>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40280432-1', 'auto');
  ga('set', 'anonymizeIp', true);
  ga('send', 'pageview');

</script>
<script>
  function trackDownload(type, url) {
    ga('send', 'event', 'download', type, url);
  }
</script>
<script src="//code.jquery.com/jquery.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script src="/assets/js/druid.js"></script>
<!-- stop page_footer include -->


    <script>
    $(function() {
      $(".toc").load("/docs/0.7.0-rc3/toc.html");

      // There is no way to tell when .gsc-input will be async loaded into the page so just try to set a placeholder until it works
      var tries = 0;
      var timer = setInterval(function() {
        tries++;
        if (tries > 300) clearInterval(timer);
        var searchInput = $('input.gsc-input');
        if (searchInput.length) {
          searchInput.attr('placeholder', 'Search');
          clearInterval(timer);
        }
      }, 100);
    });
    </script>
  </body>
</html>
