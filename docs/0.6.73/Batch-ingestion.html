<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="">
<meta name="author" content="druid">

<title>Druid | </title>
<link rel="alternate" type="application/atom+xml" href="http://druid.io/feed">
<link rel="shortcut icon" href="/img/favicon.png">

<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<link rel="stylesheet" href="//static.druid.io/web-assets/bootstrap/3.3.5/css/bootstrap.min.css">

<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700,300italic|Open+Sans:300italic,400italic,600italic,400,300,600,700' rel='stylesheet' type='text/css'>

<link rel="stylesheet" href="/css/main.css">
<link rel="stylesheet" href="/css/header.css">
<link rel="stylesheet" href="/css/footer.css">
<link rel="stylesheet" href="/css/syntax.css">
<link rel="stylesheet" href="/css/docs.css">

<script>
  (function() {
    var cx = '000162378814775985090:molvbm0vggm';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>


  </head>

  <body>
    <!-- Start page_header include -->
<div class="navbar navbar-inverse navbar-static-top druid-nav">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/"><span class="druid-logo"></span></a>
    </div>
    <div class="navbar-collapse collapse">
      <ul class="nav navbar-nav navbar-right">
        <li class=""><a href="/druid.html">Overview</a></li>
        <li class=""><a href="/downloads.html">Download</a></li>
        <li class=""><a href="/docs/0.10.1/design/index.html">Docs</a></li>
        <li class=""><a href="/community/">Contribute</a></li>
        <li class=""><a href="/blog">Blog</a></li>
        <li><a href="https://twitter.com/druidio/" class="fa fa-twitter"></a></li>
        <li><a href="https://github.com/druid-io/druid/" class="fa fa-github"></a></li>
      </ul>
    </div>
  </div>
</div>
<!-- Stop page_header include -->


    <div class="druid-header">
      <div class="container">
        <h1>Documentation</h1>
        <h4></h4>
      </div>
    </div>

    <div class="container">
      
      
      

      
      <p> Looking for the <a href="/docs/0.10.1/">latest stable documentation</a>?</p>
      

      <div class="row">
        <div class="col-md-9 doc-content">
          <p><a class="btn btn-default btn-xs visible-xs-inline-block visible-sm-inline-block" href="#toc">Table of Contents</a>
          <a class="btn btn-default btn-xs" href="http://static.druid.io/api/0.6.73">API documentation</a></p>
          <h1 id="batch-data-ingestion">Batch Data Ingestion</h1>

<p>There are two choices for batch data ingestion to your Druid cluster, you can use the <a href="Indexing-Service.html">Indexing service</a> or you can use the <code>HadoopDruidIndexer</code>.</p>

<h2 id="which-should-i-use">Which should I use?</h2>

<p>The <a href="Indexing-Service.html">Indexing service</a> is a node that can run as part of your Druid cluster and can accomplish a number of different types of indexing tasks. Even if all you care about is batch indexing, it provides for the encapsulation of things like the <a href="MySQL.html">database</a> that is used for segment metadata and other things, so that your indexing tasks do not need to include such information. The indexing service was created such that external systems could programmatically interact with it and run periodic indexing tasks. Long-term, the indexing service is going to be the preferred method of ingesting data.</p>

<p>The <code>HadoopDruidIndexer</code> runs hadoop jobs in order to separate and index data segments. It takes advantage of Hadoop as a job scheduling and distributed job execution platform. It is a simple method if you already have Hadoop running and don’t want to spend the time configuring and deploying the <a href="Indexing-Service.html">Indexing service</a> just yet.</p>

<h2 id="batch-ingestion-using-the-hadoopdruidindexer">Batch Ingestion using the HadoopDruidIndexer</h2>

<p>The HadoopDruidIndexer can be run like so:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:&lt;hadoop_config_path&gt; io.druid.cli.Main index hadoop &lt;config_file&gt;
</code></pre></div>
<p>The interval is the <a href="http://en.wikipedia.org/wiki/ISO_8601#Time_intervals">ISO8601 interval</a> of the data you are processing. The config_file is a path to a file (the &quot;specFile&quot;) that contains JSON and an example looks like:</p>
<div class="highlight"><pre><code class="language-json" data-lang="json"><span></span><span class="p">{</span>
  <span class="nt">&quot;dataSource&quot;</span><span class="p">:</span> <span class="s2">&quot;the_data_source&quot;</span><span class="p">,</span>
  <span class="nt">&quot;timestampSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;column&quot;</span><span class="p">:</span> <span class="s2">&quot;ts&quot;</span><span class="p">,</span>
    <span class="nt">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;iso, millis, posix, auto or any Joda time format&gt;&quot;</span>
  <span class="p">},</span>
  <span class="nt">&quot;dataSpec&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;csv, tsv, or json&gt;&quot;</span><span class="p">,</span>
    <span class="nt">&quot;columns&quot;</span><span class="p">:</span> <span class="p">[</span>
      <span class="s2">&quot;ts&quot;</span><span class="p">,</span>
      <span class="s2">&quot;column_1&quot;</span><span class="p">,</span>
      <span class="s2">&quot;column_2&quot;</span><span class="p">,</span>
      <span class="s2">&quot;column_3&quot;</span><span class="p">,</span>
      <span class="s2">&quot;column_4&quot;</span><span class="p">,</span>
      <span class="s2">&quot;column_5&quot;</span>
    <span class="p">],</span>
    <span class="nt">&quot;dimensions&quot;</span><span class="p">:</span> <span class="p">[</span>
      <span class="s2">&quot;column_1&quot;</span><span class="p">,</span>
      <span class="s2">&quot;column_2&quot;</span><span class="p">,</span>
      <span class="s2">&quot;column_3&quot;</span>
    <span class="p">]</span>
  <span class="p">},</span>
  <span class="nt">&quot;granularitySpec&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;uniform&quot;</span><span class="p">,</span>
    <span class="nt">&quot;intervals&quot;</span><span class="p">:</span> <span class="p">[</span>
      <span class="s2">&quot;&lt;ISO8601 interval:http:\/\/en.wikipedia.org\/wiki\/ISO_8601#Time_intervals&gt;&quot;</span>
    <span class="p">],</span>
    <span class="nt">&quot;gran&quot;</span><span class="p">:</span> <span class="s2">&quot;day&quot;</span>
  <span class="p">},</span>
  <span class="nt">&quot;pathSpec&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;granularity&quot;</span><span class="p">,</span>
    <span class="nt">&quot;dataGranularity&quot;</span><span class="p">:</span> <span class="s2">&quot;hour&quot;</span><span class="p">,</span>
    <span class="nt">&quot;inputPath&quot;</span><span class="p">:</span> <span class="s2">&quot;s3n:\/\/billy-bucket\/the\/data\/is\/here&quot;</span><span class="p">,</span>
    <span class="nt">&quot;filePattern&quot;</span><span class="p">:</span> <span class="s2">&quot;.*&quot;</span>
  <span class="p">},</span>
  <span class="nt">&quot;rollupSpec&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;aggs&quot;</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;count&quot;</span><span class="p">,</span>
        <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;event_count&quot;</span>
      <span class="p">},</span>
      <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
        <span class="nt">&quot;fieldName&quot;</span><span class="p">:</span> <span class="s2">&quot;column_4&quot;</span><span class="p">,</span>
        <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;revenue&quot;</span>
      <span class="p">},</span>
      <span class="p">{</span>
        <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;longSum&quot;</span><span class="p">,</span>
        <span class="nt">&quot;fieldName&quot;</span><span class="p">:</span> <span class="s2">&quot;column_5&quot;</span><span class="p">,</span>
        <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;clicks&quot;</span>
      <span class="p">}</span>
    <span class="p">],</span>
    <span class="nt">&quot;rollupGranularity&quot;</span><span class="p">:</span> <span class="s2">&quot;minute&quot;</span>
  <span class="p">},</span>
  <span class="nt">&quot;workingPath&quot;</span><span class="p">:</span> <span class="s2">&quot;\/tmp\/path\/on\/hdfs&quot;</span><span class="p">,</span>
  <span class="nt">&quot;segmentOutputPath&quot;</span><span class="p">:</span> <span class="s2">&quot;s3n:\/\/billy-bucket\/the\/segments\/go\/here&quot;</span><span class="p">,</span>
  <span class="nt">&quot;leaveIntermediate&quot;</span><span class="p">:</span> <span class="s2">&quot;false&quot;</span><span class="p">,</span>
  <span class="nt">&quot;partitionsSpec&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;hashed&quot;</span>
    <span class="s2">&quot;targetPartitionSize&quot;</span><span class="p">:</span> <span class="mi">5000000</span>
  <span class="p">},</span>
  <span class="nt">&quot;updaterJobSpec&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;db&quot;</span><span class="p">,</span>
    <span class="nt">&quot;connectURI&quot;</span><span class="p">:</span> <span class="s2">&quot;jdbc:mysql:\/\/localhost:7980\/test_db&quot;</span><span class="p">,</span>
    <span class="nt">&quot;user&quot;</span><span class="p">:</span> <span class="s2">&quot;username&quot;</span><span class="p">,</span>
    <span class="nt">&quot;password&quot;</span><span class="p">:</span> <span class="s2">&quot;passmeup&quot;</span><span class="p">,</span>
    <span class="nt">&quot;segmentTable&quot;</span><span class="p">:</span> <span class="s2">&quot;segments&quot;</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="hadoop-index-config">Hadoop Index Config</h3>

<table><thead>
<tr>
<th>property</th>
<th>description</th>
<th>required?</th>
</tr>
</thead><tbody>
<tr>
<td>dataSource</td>
<td>name of the dataSource the data will belong to</td>
<td>yes</td>
</tr>
<tr>
<td>timestampSpec</td>
<td>includes the column that is to be used as the timestamp column and the format of the timestamps; auto = either iso or millis, Joda time formats can be found <a href="http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html">here</a>.</td>
<td>yes</td>
</tr>
<tr>
<td>dataSpec</td>
<td>a specification of the data format and an array that names all of the columns in the input data.</td>
<td>yes</td>
</tr>
<tr>
<td>dimensions</td>
<td>the columns that are to be used as dimensions.</td>
<td>yes</td>
</tr>
<tr>
<td>granularitySpec</td>
<td>the time granularity and interval to chunk segments up into.</td>
<td>yes</td>
</tr>
<tr>
<td>pathSpec</td>
<td>a specification of where to pull the data in from</td>
<td>yes</td>
</tr>
<tr>
<td>rollupSpec</td>
<td>a specification of the rollup to perform while processing the data</td>
<td>yes</td>
</tr>
<tr>
<td>workingPath</td>
<td>the working path to use for intermediate results (results between Hadoop jobs).</td>
<td>yes</td>
</tr>
<tr>
<td>segmentOutputPath</td>
<td>the path to dump segments into.</td>
<td>yes</td>
</tr>
<tr>
<td>leaveIntermediate</td>
<td>leave behind files in the workingPath when job completes or fails (debugging tool).</td>
<td>no</td>
</tr>
<tr>
<td>partitionsSpec</td>
<td>a specification of how to partition each time bucket into segments, absence of this property means no partitioning will occur.</td>
<td>no</td>
</tr>
<tr>
<td>updaterJobSpec</td>
<td>a specification of how to update the metadata for the druid cluster these segments belong to.</td>
<td>yes</td>
</tr>
</tbody></table>

<h3 id="path-specification">Path specification</h3>

<p>There are multiple types of path specification:</p>

<h5 id="granularity"><code>granularity</code></h5>

<p>Is a type of data loader that expects data to be laid out in a specific path format. Specifically, it expects it to be segregated by day in this directory format <code>y=XXXX/m=XX/d=XX/H=XX/M=XX/S=XX</code> (dates are represented by lowercase, time is represented by uppercase).</p>

<table><thead>
<tr>
<th>property</th>
<th>description</th>
<th>required?</th>
</tr>
</thead><tbody>
<tr>
<td>dataGranularity</td>
<td>specifies the granularity to expect the data at, e.g. hour means to expect directories <code>y=XXXX/m=XX/d=XX/H=XX</code>.</td>
<td>yes</td>
</tr>
<tr>
<td>inputPath</td>
<td>Base path to append the expected time path to.</td>
<td>yes</td>
</tr>
<tr>
<td>filePattern</td>
<td>Pattern that files should match to be included.</td>
<td>yes</td>
</tr>
</tbody></table>

<p>For example, if the sample config were run with the interval 2012-06-01/2012-06-02, it would expect data at the paths</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=00
s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=01
...
s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=23
</code></pre></div>
<h3 id="rollup-specification">Rollup specification</h3>

<p>The indexing process has the ability to roll data up as it processes the incoming data. If data has already been summarized, summarizing it again will produce the same results so either way is not a problem. This specifies how that rollup should take place.</p>

<table><thead>
<tr>
<th>property</th>
<th>description</th>
<th>required?</th>
</tr>
</thead><tbody>
<tr>
<td>aggs</td>
<td>specifies a list of aggregators to aggregate for each bucket (a bucket is defined by the tuple of the truncated timestamp and the dimensions). Aggregators available here are the same as available when querying.</td>
<td>yes</td>
</tr>
<tr>
<td>rollupGranularity</td>
<td>The granularity to use when truncating incoming timestamps for bucketization.</td>
<td>yes</td>
</tr>
</tbody></table>

<h3 id="partitioning-specification">Partitioning specification</h3>

<p>Segments are always partitioned based on timestamp (according to the granularitySpec) and may be further partitioned in some other way depending on partition type.
Druid supports two types of partitions spec - singleDimension and hashed.</p>

<p>In SingleDimension partition type data is partitioned based on the values in that dimension.
For example, data for a day may be split by the dimension &quot;last_name&quot; into two segments: one with all values from A-M and one with all values from N-Z.</p>

<p>In hashed partition type, the number of partitions is determined based on the targetPartitionSize and cardinality of input set and the data is partitioned based on the hashcode of the row.</p>

<p>It is recommended to use Hashed partition as it is more efficient than singleDimension since it does not need to determine the dimension for creating partitions.
Hashing also gives better distribution of data resulting in equal sized partitons and improving query performance</p>

<p>To use this option, the indexer must be given a target partition size. It can then find a good set of partition ranges on its own.</p>

<table><thead>
<tr>
<th>property</th>
<th>description</th>
<th>required?</th>
</tr>
</thead><tbody>
<tr>
<td>type</td>
<td>type of partitionSpec to be used</td>
<td>no, default : singleDimension</td>
</tr>
<tr>
<td>targetPartitionSize</td>
<td>target number of rows to include in a partition, should be a number that targets segments of 700MB~1GB.</td>
<td>yes</td>
</tr>
<tr>
<td>partitionDimension</td>
<td>the dimension to partition on. Leave blank to select a dimension automatically.</td>
<td>no</td>
</tr>
<tr>
<td>assumeGrouped</td>
<td>assume input data has already been grouped on time and dimensions. This is faster, but can choose suboptimal partitions if the assumption is violated.</td>
<td>no</td>
</tr>
</tbody></table>

<h3 id="updater-job-spec">Updater job spec</h3>

<p>This is a specification of the properties that tell the job how to update metadata such that the Druid cluster will see the output segments and load them.</p>

<table><thead>
<tr>
<th>property</th>
<th>description</th>
<th>required?</th>
</tr>
</thead><tbody>
<tr>
<td>type</td>
<td>&quot;db&quot; is the only value available.</td>
<td>yes</td>
</tr>
<tr>
<td>connectURI</td>
<td>A valid JDBC url to MySQL.</td>
<td>yes</td>
</tr>
<tr>
<td>user</td>
<td>Username for db.</td>
<td>yes</td>
</tr>
<tr>
<td>password</td>
<td>password for db.</td>
<td>yes</td>
</tr>
<tr>
<td>segmentTable</td>
<td>Table to use in DB.</td>
<td>yes</td>
</tr>
</tbody></table>

<p>These properties should parrot what you have configured for your <a href="Coordinator.html">Coordinator</a>.</p>

<h2 id="batch-ingestion-using-the-indexing-service">Batch Ingestion Using the Indexing Service</h2>

<p>Batch ingestion for the indexing service is done by submitting a <a href="Tasks.html">Hadoop Index Task</a>. The indexing service can be started by issuing:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>java -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:config/overlord io.druid.cli.Main server overlord
</code></pre></div>
<p>This will start up a very simple local indexing service. For more complex deployments of the indexing service, see <a href="Indexing-Service.html">here</a>.</p>

<p>The schema of the Hadoop Index Task contains a task &quot;type&quot; and a Hadoop Index Config. A sample Hadoop index task is shown below:</p>
<div class="highlight"><pre><code class="language-json" data-lang="json"><span></span><span class="p">{</span>
  <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;index_hadoop&quot;</span><span class="p">,</span>
  <span class="nt">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;dataSource&quot;</span> <span class="p">:</span> <span class="s2">&quot;example&quot;</span><span class="p">,</span>
    <span class="nt">&quot;timestampSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;column&quot;</span> <span class="p">:</span> <span class="s2">&quot;timestamp&quot;</span><span class="p">,</span>
      <span class="nt">&quot;format&quot;</span> <span class="p">:</span> <span class="s2">&quot;auto&quot;</span>
    <span class="p">},</span>
    <span class="nt">&quot;dataSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;format&quot;</span> <span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span>
      <span class="nt">&quot;dimensions&quot;</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;dim1&quot;</span><span class="p">,</span><span class="s2">&quot;dim2&quot;</span><span class="p">,</span><span class="s2">&quot;dim3&quot;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="nt">&quot;granularitySpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;uniform&quot;</span><span class="p">,</span>
      <span class="nt">&quot;gran&quot;</span> <span class="p">:</span> <span class="s2">&quot;DAY&quot;</span><span class="p">,</span>
      <span class="nt">&quot;intervals&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;2013-08-31/2013-09-01&quot;</span> <span class="p">]</span>
    <span class="p">},</span>
    <span class="nt">&quot;pathSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;static&quot;</span><span class="p">,</span>
      <span class="nt">&quot;paths&quot;</span> <span class="p">:</span> <span class="s2">&quot;data.json&quot;</span>
    <span class="p">},</span>
    <span class="nt">&quot;targetPartitionSi:qze&quot;</span> <span class="p">:</span> <span class="mi">5000000</span><span class="p">,</span>
    <span class="nt">&quot;rollupSpec&quot;</span> <span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;aggs&quot;</span><span class="p">:</span> <span class="p">[{</span>
          <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;count&quot;</span><span class="p">,</span>
          <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;count&quot;</span>
        <span class="p">},</span> <span class="p">{</span>
          <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
          <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;added&quot;</span><span class="p">,</span>
          <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;added&quot;</span>
        <span class="p">},</span> <span class="p">{</span>
          <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
          <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;deleted&quot;</span><span class="p">,</span>
          <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;deleted&quot;</span>
        <span class="p">},</span> <span class="p">{</span>
          <span class="nt">&quot;type&quot;</span> <span class="p">:</span> <span class="s2">&quot;doubleSum&quot;</span><span class="p">,</span>
          <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;delta&quot;</span><span class="p">,</span>
          <span class="nt">&quot;fieldName&quot;</span> <span class="p">:</span> <span class="s2">&quot;delta&quot;</span>
      <span class="p">}],</span>
      <span class="nt">&quot;rollupGranularity&quot;</span> <span class="p">:</span> <span class="s2">&quot;none&quot;</span>
    <span class="p">}</span>
  <span class="p">}</span>
</code></pre></div>
<table><thead>
<tr>
<th>property</th>
<th>description</th>
<th>required?</th>
</tr>
</thead><tbody>
<tr>
<td>type</td>
<td>This should be &quot;index_hadoop&quot;.</td>
<td>yes</td>
</tr>
<tr>
<td>config</td>
<td>A Hadoop Index Config (see above).</td>
<td>yes</td>
</tr>
<tr>
<td>hadoopCoordinates</td>
<td>The Maven <code>&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;</code> of Hadoop to use. The default is &quot;org.apache.hadoop:hadoop-core:1.0.3&quot;.</td>
<td>no</td>
</tr>
</tbody></table>

<p>The Hadoop Index Config submitted as part of an Hadoop Index Task is identical to the Hadoop Index Config used by the <code>HadoopBatchIndexer</code> except that three fields must be omitted: <code>segmentOutputPath</code>, <code>workingPath</code>, <code>updaterJobSpec</code>. The Indexing Service takes care of setting these fields internally.</p>

<p>To run the task:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>curl -X &#39;POST&#39; -H &#39;Content-Type:application/json&#39; -d @example_index_hadoop_task.json localhost:8087/druid/indexer/v1/task
</code></pre></div>
<p>If the task succeeds, you should see in the logs of the indexing service:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"><span></span>2013-10-16 16:38:31,945 INFO [pool-6-thread-1] io.druid.indexing.overlord.exec.TaskConsumer - Task SUCCESS: HadoopIndexTask...
</code></pre></div>
<h2 id="having-problems">Having Problems?</h2>

<p>Getting data into Druid can definitely be difficult for first time users. Please don&#39;t hesitate to ask questions in our IRC channel or on our <a href="https://groups.google.com/forum/#!forum/druid-development">google groups page</a>.</p>

        </div>
        <div class="col-md-3">
          <div class="searchbox">
            <gcse:searchbox-only></gcse:searchbox-only>
          </div>
          <div id="toc" class="nav toc hidden-print">
          </div>
        </div>
      </div>
    </div>

    <!-- Start page_footer include -->
<footer class="druid-footer">
<div class="container">
  <div class="text-center">
    <p>
    <a href="/community/">Community</a>&ensp;·&ensp;
    <a href="/downloads.html">Download</a>&ensp;·&ensp;
    <a href="/druid-powered.html">Powered by Druid</a>&ensp;·&ensp;
    <a href="/faq.html">FAQ</a>&ensp;·&ensp;
    <a href="/licensing.html">License</a>
    </p>
  </div>
  <div class="text-center">
    <a href="https://groups.google.com/forum/#!forum/druid-user"><span class="fa fa-lg fa-comments"></span></a>&ensp;·&ensp;
    <a href="https://twitter.com/druidio"><span class="fa fa-lg fa-twitter"></span></a>&ensp;·&ensp;
    <a href="https://github.com/druid-io/druid"><span class="fa fa-lg fa-github"></span></a>
  </div>
  <div class="text-center license">
    Except where otherwise noted, licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>
  </div>
</div>
</footer>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40280432-1', 'auto');
  ga('set', 'anonymizeIp', true);
  ga('send', 'pageview');

</script>
<script src="http://code.jquery.com/jquery.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script src="/assets/js/druid.js"></script>
<!-- stop page_footer include -->


    <script>
    $(function() {
      $(".toc").load("/docs/0.6.73/toc.html");

      // There is no way to tell when .gsc-input will be async loaded into the page so just try to set a placeholder until it works
      var tries = 0;
      var timer = setInterval(function() {
        tries++;
        if (tries > 300) clearInterval(timer);
        var searchInput = $('input.gsc-input');
        if (searchInput.length) {
          searchInput.attr('placeholder', 'Search');
          clearInterval(timer);
        }
      }, 100);
    });
    </script>
  </body>
</html>
